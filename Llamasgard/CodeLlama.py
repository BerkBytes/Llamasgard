from typing import List, Mapping, Optional, Any
from langchain.llms.base import LLM
import requests

API_URL = "http://127.0.0.1:8000/predict"

class CodeLlama(LLM):
    """
    A custom LLM class that integrates the CodeLlama model.

    Arguments:

    system: (str) Default system message
    prefix: (str) Prefix for each generation
    max_new_tokens: (int) The maximum numbers of tokens to generate
    temperature: (float) Temperature to use for sampling
    top_p: (float) The top-p value to use for sampling
    top_k: (int) The top k values use for sampling
    num_return_sequences: (int) Number of sequences to return
    repetition_penalty: (float) The penalty to apply repeated tokens
    """
    # Default parameters
    prefix: str = ''
    max_new_tokens: int = 128
    system: str = f"""Your name is Code Llama, a state-of-the-art large language model (LLM) specialized in generating and discussing code. You are an advanced descendant of Llama 2. Keep responses concise and be friendly towards users. Be sure to completely answer the request within {max_new_tokens} tokens.""",
    temperature: float = 0.9
    top_p: float = 0.95
    repetition_penalty: float = 1.2
    top_k: int = 50
    num_return_sequences: int = 1

    def __init__(self, **kwargs):
        super(CodeLlama, self).__init__()
        for key, value in kwargs.items():
            setattr(self, key, value)

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get all the identifying parameters."""
        return {
            'API_URL': API_URL,
            'system': self.system,
            'prefix': self.prefix,
            'max_new_tokens': self.max_new_tokens,
            'temperature': self.temperature,
            'top_p': self.top_p,
            'repetition_penalty': self.repetition_penalty,
            'top_k': self.top_k,
            'num_return_sequences': self.num_return_sequences
        }

    @property
    def _llm_type(self) -> str:
        return 'CodeLlama'

    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        """
        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered

        Returns:
            The string generated by the model
        """
        
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")
        
        data = {
            'user': prompt,
            'system': kwargs.get('system', self.system),
            'prefix': kwargs.get('prefix', self.prefix),
            'max_new_tokens': kwargs.get('max_new_tokens', self.max_new_tokens),
            'temperature': kwargs.get('temperature', self.temperature),
            'top_p': kwargs.get('top_p', self.top_p),
            'repetition_penalty': kwargs.get('repetition_penalty', self.repetition_penalty),
            'top_k': kwargs.get('top_k', self.top_k),
            'num_return_sequences': kwargs.get('num_return_sequences', self.num_return_sequences),
        }

        
        response = requests.post(API_URL, json=data)
        
        if response.status_code == 200:
            return response.json()['response']
        else:
            return f"Error {response.status_code}: {response.text}"

